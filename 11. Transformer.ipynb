{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2137ac01-0128-4853-aa68-75a50cfb94e7",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "- Transformer는 **Self-Attention** 메커니즘을 활용해 입력 시퀀스 내 요소들 간의 관계를 동시 처리하는 딥러닝 모델\n",
    "\n",
    "- 인코더-디코더 구조를 갖고 있으며, 각각 다층의 attention 및 feedforward 레이어로 구성\n",
    "\n",
    "- 앞 단원에서는 순환 신경망을 거친 encoder output과 마찬가지로 순환 신경망을 거쳐 나온 디코더 아웃풋 간의 attention 구조를 살펴보았다.\n",
    "\n",
    "- Transformer는 순환 신경망 과정을 생략하고 대신 self-attention 구조를 이용.\n",
    "\n",
    "- 순차적인 처리 없이도 병렬 연산이 가능해, 번역·요약·질문응답 등 다양한 자연어 처리 작업에서 뛰어난 성능을 보임\n",
    "\n",
    "- 과목 특성상 자연어 처리작업을 하기는 어려우므로 이 단원에서는 Transformer의 구조에 대해 간략히 알아본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d064e78e-b453-4c4a-93d3-72d21b9a3dce",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "Transformer 아키텍처는 기본적으로 순서에 의존하지 않는 구조인 self-attention 메커니즘을 기반으로 한다.   \n",
    "    \n",
    "추후 알아볼 self-attention은 입력 시퀀스를 처리할 때 각 위치의 값들이 서로 독립적으로 처리되므로, 순서 정보가 내재되어 있지 않다. \n",
    "- RNN과 같은 전통적인 시퀀스 모델들은 자연스럽게 순서를 처리하지만, Transformer는 병렬 처리가 가능하도록 설계되어 순서 정보가 내재되어 있지 않다.\n",
    "\n",
    "따라서, 시퀀스의 순서 정보를 모델에 제공하기 위해 positional encoding을 도입하여, 입력 시퀀스의 각 위치에 대한 순서 정보를 학습할 수 있게 하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f6914-a659-4c25-af83-09f66066fcf1",
   "metadata": {},
   "source": [
    "### Positional Encoding 방식\n",
    "\n",
    "Transformer에서 사용되는 대표적인 positional encoding 방식은 사인과 코사인 함수를 사용한다. \n",
    "\n",
    "각 차원에 대해 다른 주기를 가지는 사인과 코사인 값을 사용하여 각 위치의 인코딩을 생성한다.\n",
    "\n",
    "이러한 방식은 시퀀스의 길이에 관계없이 위치 정보를 제공할 수 있고, 모델이 이를 통해 위치 정보를 학습할 수 있도록 돕는다.\n",
    "\n",
    "Positional enconding에 사용되는 수식은 다음과 같다.\n",
    "\n",
    "$$\\textrm{PE}{(p, 2i)} = \\sin\\left(\\frac{p}{N^{2i/d}} \\right), \\quad \\textrm{PE}{(p, 2i+1)} = \\cos\\left(\\frac{p}{N^{2i/d}}\\right)$$\n",
    "\n",
    "- $p$ : 시간축에 대한 인덱스\n",
    "- $2i, 2i+1$ : 입력 시퀀스의 차원에 대한 인덱스, 각, 짝수와 홀수 인덱스를 의미\n",
    "- $N$ : 큰 양수\n",
    "- $d$ : 입력 시퀀스의 차원\n",
    "\n",
    "`numpy`를 이용해 간단히 표현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473d0130-9fbf-41db-88d8-4d0294f71ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          1.          0.          1.        ]\n",
      " [ 0.84147098  0.54030231  0.00999983  0.99995   ]\n",
      " [ 0.90929743 -0.41614684  0.01999867  0.99980001]\n",
      " [ 0.14112001 -0.9899925   0.0299955   0.99955003]\n",
      " [-0.7568025  -0.65364362  0.03998933  0.99920011]\n",
      " [-0.95892427  0.28366219  0.04997917  0.99875026]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Define sequence length and model dimension\n",
    "timestep = 6\n",
    "dim = 4\n",
    "\n",
    "pe = np.zeros((timestep, dim))\n",
    "    \n",
    "# Calculate the position indices for the sequence\n",
    "position = np.arange(0, timestep)[:, np.newaxis]\n",
    "\n",
    "N = 10000.0\n",
    "# Calculate the dimension indices\n",
    "div_term = 1 / np.power(N, np.arange(0, dim, 2) / dim)\n",
    "\n",
    "# Apply the sin function to even indices and cos function to odd indices\n",
    "pe[:, 0::2] = np.sin(position * div_term)  # Apply sin to even indices\n",
    "pe[:, 1::2] = np.cos(position * div_term)  # Apply cos to odd indices\n",
    "\n",
    "# Print the positional encoding matrix\n",
    "print(pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af97fb29-29b9-4e97-a531-45060bea6b8a",
   "metadata": {},
   "source": [
    "위에서 출력된 행렬의 행은 시간축이고, 열은 입력 시퀀스의 차원을 나타낸다.\n",
    "\n",
    "같은 거리에 있는 positional encoding 벡터들을 내적하였을 때, 동일한 값을 가짐을 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1966770f-8aaa-493d-ae1c-978b4af60cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.540252306284805,\n",
       " 1.5402523062848048,\n",
       " 1.540252306284805,\n",
       " 1.540252306284805,\n",
       " 1.540252306284805)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[0, :] @ pe[1, :], pe[1, :] @ pe[2, :], pe[2, :] @ pe[3, :], pe[3, :] @ pe[4, :], pe[4, :] @ pe[5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec15fe38-3f74-4c22-b91d-c27075f5d842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5836531701194354,\n",
       " 0.5836531701194354,\n",
       " 0.5836531701194354,\n",
       " 0.5836531701194355)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[0, :] @ pe[2, :], pe[1, :] @ pe[3, :], pe[2, :] @ pe[4, :], pe[3, :] @ pe[5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377816c5-18ae-43cd-b2b5-0486ed27de1c",
   "metadata": {},
   "source": [
    "Positional encoding을 담당하는 keras layer를 클래스로 정의하여 보자.\n",
    "\n",
    "- `class PositionalEncoding(layers.Layer):` TensorFlow/Keras에서 사용자 정의 레이어를 만들기 위한 정의\n",
    "\n",
    "  - `layers.Layer`를 상속했기 때문에 Keras의 커스텀 레이어가 됨\n",
    " \n",
    "  - 이 레이어는 `__init__`, `call`, 등의 메서드를 통해 정의된 사용자 정의 동작을 수행할 수 있음\n",
    "\n",
    "- 편의상, 이 코드는 짝수의 `dim`에 대해서만 작동하도록 만들었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6005d577-0923-4cc9-abf9-bd0923b812e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_len, dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pos_encoding = np.zeros((max_len, dim))\n",
    "        positions = np.arange(0, max_len)[:, np.newaxis]\n",
    "\n",
    "        N = 10000.0\n",
    "        div_term =  1 / np.power(N, np.arange(0, dim, 2) / dim)\n",
    "        \n",
    "        pos_encoding[:, 0::2] = np.sin(positions * div_term)\n",
    "        pos_encoding[:, 1::2] = np.cos(positions * div_term)\n",
    "        \n",
    "        pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "        self.pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6435997-67ed-46fc-872f-b30f42a31cfb",
   "metadata": {},
   "source": [
    "잘 작동하는지 확인해 보자.\n",
    "\n",
    "- shape이 `(1, timestep, dim)`인 제로 시퀀스 입력에 대해 위치 정보를 부여하는 positional encoding이 잘 작동하는지 테스트\n",
    "\n",
    "- 아래 출력 결과를 보면, 출력이 timestep마다 다르며, 이는 positional encoding이 위치 정보를 명시적으로 부여했다는 의미 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72fb5f22-a33e-40f3-bf37-f81fd6ef17f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6, 4), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  1.        ,  0.        ,  1.        ],\n",
       "        [ 0.84147096,  0.5403023 ,  0.00999983,  0.99995   ],\n",
       "        [ 0.9092974 , -0.41614684,  0.01999867,  0.9998    ],\n",
       "        [ 0.14112   , -0.9899925 ,  0.0299955 ,  0.99955004],\n",
       "        [-0.7568025 , -0.6536436 ,  0.03998933,  0.9992001 ],\n",
       "        [-0.9589243 ,  0.2836622 ,  0.04997917,  0.99875027]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PositionalEncoding(timestep, dim)(np.zeros((1, timestep, dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e79979-b9af-4fdd-ac38-fbf389370e11",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "\n",
    "Attention은 이전 단원에서 살펴본 것처럼 query, key, value들의 dot product들로 계산한다.\n",
    "\n",
    "Attention의 목적은 query와 key의 유사성을 dot-product로 계산하여, 이를 기반으로 value에 가중치를 부여하는 것이다.\n",
    "\n",
    "* **Query (Q)**: 현재 우리가 “관심 있는 항목”에 대한 표현. 디코더의 현재 시점 상태 등이 사용됨.\n",
    "  \n",
    "* **Key (K)**: 인코더의 각 위치에 대한 설명. 디코더가 어떤 인코더 위치에 주목할지 판단할 기준.\n",
    "  \n",
    "* **Value (V)**: 실제로 정보를 얻고자 하는 대상. 보통 Key와 동일한 위치에서 생성된 벡터.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6b969-a478-48ed-958c-3dbd37b85b3d",
   "metadata": {},
   "source": [
    "### 가중치 행렬 연산\n",
    "\n",
    "Transformer의 Attention 메커니즘에서 Query, Key, Value는 각각 입력 시퀀스 `X_q`, `X_k`, `X_v`에 가중치 행렬 `W_q`, `W_k`, `W_v`를 곱하여 생성된다.\n",
    "\n",
    "- 만약 쿼리 입력 시퀀스 `X_q`와 키 입력 시퀀스 `X_k`가 같다면 self-attention이라고 부른다. \n",
    "\n",
    "- 쿼리 입력 시퀀스 `X_q`와 값 입력 시퀀스 `X_v`는 같을 수도 있고 다를 수도 있다.\n",
    "\n",
    "쿼리 입력 시퀀스의 차원을 `d_model_q`, 키 입력 시퀀스의 차원을 `d_model_q`이라고 하자. \n",
    "\n",
    "- 쿼리 입력 시퀀스의 shape : `(batch_size, seq_len_q, d_model_q)`\n",
    "\n",
    "- 키 입력 시퀀스의 shape : `(batch_size, seq_len_k, d_model_k)`\n",
    "\n",
    "가중치 행렬들의 shape은 다음과 같다.\n",
    "\n",
    "* `W_q`: `(d_model_q, key_dim)`  \n",
    "* `W_k`: `(d_model_k, key_dim)`  \n",
    "* `W_v`: `(d_model_q, val_dim)`  \n",
    "\n",
    "입력 시퀀스를 `X`들이라고 할 때, query, key, value의 계산 과정과 shape들은 다음과 같다.\n",
    "\n",
    "* `Q = X_q @ W_q`    # shape: `(batch_size, seq_len_q, key_dim)`\n",
    "* `K = X_k @ W_k`    # shape: `(batch_size, seq_len_k, key_dim)`\n",
    "* `V = X_v @ W_v`    # shape: `(batch_size, seq_len_k, val_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2825d5bb-9546-419d-a744-8a3220e3cb15",
   "metadata": {},
   "source": [
    "### 차원 관계 정리\n",
    "\n",
    "| 입력 `X`          | `W`                                | 쿼리, 키, 값 |\n",
    "| ------------- | --------------------------------- |-----------------|\n",
    "| `X_q` : `(batch_size, seq_len_q, d_model_q)` | `W_q` : `(d_model_q, key_dim)` | `Q` :  `(batch_size, seq_len_q, key_dim)`\n",
    "| `X_k` : `(batch_size, seq_len_k, d_model_k)` | `W_k`: `(d_model_k, key_dim)` | `K` : `(batch_size, seq_len_k, key_dim)`\n",
    "| `X_v` : `(batch_size, seq_len_k, d_model_v)` | `W_v`: `(d_model_q, val_dim)` | `V` : `(batch_size, seq_len_k, val_dim)`\n",
    "\n",
    "\n",
    "* `key_dim == query_dim`이어야 $\\mathrm{Q} \\, \\mathrm{K}^{\\top}$가 정의됨.\n",
    "  \n",
    "* `seq_len_k`와 `seq_len_q`는 시퀀스의 길이이며 같을 수도 있고, 다를 수도 있음. (ex: 인코더는 10, 디코더는 5)\n",
    "  \n",
    "* key의 dimension(`key_dim`)과 value의 dimension(`val_dim`)은 같을 수도 있고, 다를 수도 있음\n",
    "\n",
    "* `d_model_q`, `d_model_k`, `d_model_v` 또한 모두 같을 수도 있고 다를 수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3706355-92eb-45eb-9d66-c30cf55cfb4b",
   "metadata": {},
   "source": [
    "### Attention 연산 요약\n",
    "\n",
    "1. 유사도 계산 (Dot Product)\n",
    "   - $\\text{scores} = \\mathrm{Q} \\, \\mathrm{K}^{\\top}, \\quad$ shape:  `(batch_size, seq_len_q, seq_len_k)` <br><br>\n",
    "\n",
    "2. Scaling (정규화)\n",
    "   - $\\text{scaled\\_scores} = \\frac{\\mathrm{Q} \\, \\mathrm{K}^{\\top}}{\\sqrt{\\text{key\\_dim}}}$ <br><br>\n",
    "\n",
    "3. Softmax로 확률 분포화 (각 query 시점 마다)\n",
    "   - $\\text{Attention Weights} = \\text{softmax}(\\text{scaled\\_scores})$ <br><br>\n",
    "\n",
    "4. Weighted sum of values \n",
    "   - $\\text{context} = \\text{Attention Weights} \\cdot \\mathrm{V}, \\quad $  shape: `(batch_size, seq_len_q, val_dim)`\n",
    "\n",
    "#### Scaling을 하는 이유\n",
    "\n",
    "- Dot product attention은 종종 `key_dim`(depth)의 제곱근으로 조정되는데, 이는 model의 depth가 클 경우 dot product의 값이 매우 커질 수 있기 때문이다. \n",
    "\n",
    "- 이렇게 되면 softmax function이 매우 좁은 범위에서 작동하게 되어, gradient가 작은 부분에서 매우 급격한 변화가 발생하는 'sharp' softmax가 생성될 수 있다.\n",
    "\n",
    "- 이러한 문제를 피하기 위해 depth의 제곱근으로 dot product 값을 나누어 조정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d26ee-51ca-4416-9dc4-d77f13b07fa5",
   "metadata": {},
   "source": [
    "아래 예제에서 `Q`, `K`, `V`는 `W`들과의 연산을 통해 이미 만들어진 query, key, value의 계산 결과라고 가정하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58700937-6949-4e51-9a0b-a352fb4aa333",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "Q = np.array([[0, 0, 10],\n",
    "                   [0, 10, 0],\n",
    "                   [10, 10, 0]])  # (seq_len_q, key_dim) = (3, 3)\n",
    "\n",
    "K = np.array([[10, 0, 0],\n",
    "                   [0, 10, 0],\n",
    "                   [0, 0, 10],\n",
    "                   [0, 0, 10]])  # (seq_len_k, key_dim) = (4, 3)\n",
    "\n",
    "V = np.array([[1, 0],\n",
    "                   [10, 0],\n",
    "                   [100, 5],\n",
    "                   [1000, 6]])  # (seq_len_k, val_dim) = (4, 2)\n",
    "\n",
    "key_dim = np.shape(K)[-1] # 3\n",
    "query_dim = np.shape(Q)[-1] # 3\n",
    "\n",
    "seq_len_q = np.shape(Q)[-2]  # 3\n",
    "seq_len_k = np.shape(K)[-2]  # 4\n",
    "seq_len_v = np.shape(V)[-2]  # 4\n",
    "\n",
    "\n",
    "matmul_qk = Q @ K.T   # attention score, (seq_len_q, seq_len_k) = (3, 4)\n",
    "\n",
    "scaled_attention_logits = matmul_qk / math.sqrt(key_dim) # scaled attention score\n",
    "\n",
    "attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (seq_len_q, seq_len_k) = (3, 4)\n",
    "\n",
    "output = tf.matmul(attention_weights, V)  # (seq_len_q, depth_v) = (3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557e824-9b1e-46fb-8e6c-d450cb1d6ffe",
   "metadata": {},
   "source": [
    "아래 코드에서 `attention_weights`을 프린트한 결과를 보면, 주어진 query가 value의 어디에 집중해야 하는지 설명한다.\n",
    "\n",
    "- Query `temp_q`의 첫번째 값은 value의 세번째와 네번째 값에 집중해야 한다.\n",
    "- Query `temp_q`의 두번째 값은 value의 두번째 값에 집중해야 한다.\n",
    "- Query `temp_q`의 세번째 값은 value의 첫번째와 두번째 값에 집중해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e80ba4-c9e0-498b-af8a-5c6ad4450ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weight : \n",
      " tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Attention weight : \\n\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583b705-e8c5-4f90-9700-bdfb2c7f4481",
   "metadata": {},
   "source": [
    "Attention weight를 value에 적용된 결과는 다음과 같다.\n",
    "\n",
    "- Query `Q`의 첫번째 값의 의미는 `[550.    5.5]`으로 해석됨.\n",
    "- Query `Q`의 두번째 값의 의미는 `[ 10.    0. ]`으로 해석됨.\n",
    "- Query `Q`의 세번째 값의 의미는 `[  5.5   0. ]`으로 해석됨.\n",
    "\n",
    "Output의 sequence 길이는 `seq_len_q`와 같고, dimension은 `dim_v`와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddd3da9f-3e58-4eb4-9918-31fd2ddb88e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output  : \n",
      " tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Output  : \\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c577e-46d3-4abc-a058-c8e2939a8395",
   "metadata": {},
   "source": [
    "## Multi-head attention\n",
    "\n",
    "**Multi-Head Attention**에서는 \n",
    "\n",
    "* 쿼리 Q, 키 K, 값 V의 차원을 `num_heads`개의 subspace로 분할하고, 각 head마다 독립적인 attention을 수행한 뒤,\n",
    "\n",
    "* 그 결과들을 **결합(concatenate)** 후 다시 선형 변환하여 하나의 출력으로 만든다.\n",
    "\n",
    "각, head별 쿼리와 키의 차원을 `key_dim`이라 하자.\n",
    "\n",
    "그러면 `key_dim * num_head`는 `Q` 전체의 feature 차원의 개수가 된다. \n",
    "\n",
    "예를 들어, `Q` 전체의 feature 차원이 `512`, `num_heads = 8`이면, 각 head에서 키와 쿼리는 `512 / 8 = 64`차원짜리 feature를 갖게 되는 셈."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28133048-f344-41d1-bfd9-5f734d2569b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.25       0.25       0.25       0.25      ]\n",
      "  [0.         1.         0.         0.        ]\n",
      "  [0.5        0.5        0.         0.        ]]\n",
      "\n",
      " [[0.         0.33333333 0.33333333 0.33333333]\n",
      "  [0.25       0.25       0.25       0.25      ]\n",
      "  [0.25       0.25       0.25       0.25      ]]], shape=(2, 3, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "Q = np.array([[0, 0, 10, 10],\n",
    "                   [0, 10, 0, 0],\n",
    "                   [10, 10, 0, 0]])  # (4, 3)\n",
    "\n",
    "K = np.array([[10, 0, 0, 0],\n",
    "                   [0, 10, 0, 10],\n",
    "                   [0, 0, 10, 0],\n",
    "                   [0, 0, 10, 0]])  # (4, 4)\n",
    "\n",
    "V = np.array([[1, 0],\n",
    "                   [10, 0],\n",
    "                   [100, 5],\n",
    "                   [1000, 6]])  # (4, 2)\n",
    "\n",
    "\n",
    "total_key_dim = np.shape(K)[-1] # 4\n",
    "total_v_dim = np.shape(V)[-1] # 2\n",
    "num_heads = 2\n",
    "\n",
    "key_dim = total_key_dim// num_heads\n",
    "v_dim = total_v_dim // num_heads\n",
    "\n",
    "Q = np.reshape(Q, (-1, num_heads, key_dim)) # shape = (seq_len, num_heads, key_dim)\n",
    "Q = Q.transpose((1, 0, 2)) # shape = (num_heads, seq_len, key_dim)\n",
    "\n",
    "K = np.reshape(K, (-1, num_heads, key_dim))\n",
    "K = K.transpose((1, 0, 2))\n",
    "\n",
    "V = np.reshape(V, (-1, num_heads, v_dim))\n",
    "V = V.transpose((1, 0, 2))\n",
    "\n",
    "matmul_qk = np.matmul(Q, K.transpose((0, 2, 1)))   # attention score\n",
    "\n",
    "scaled_attention_logits = matmul_qk / math.sqrt(key_dim) # scaled attention score, (..., seq_len_q, seq_len_k)\n",
    "\n",
    "attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "print(attention_weights)  # num_heads 개의 attention_weights들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e92a56-8480-48d1-985e-229adc3f9289",
   "metadata": {},
   "source": [
    "최종 output, 즉, 쿼리 별 value 결과값은 이전과 마찬가지로 `(seq_len_q, dim_v)`의 shape을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1814d7fc-5741-441f-96cf-3015ad02c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[277.75      ]\n",
      "  [ 10.        ]\n",
      "  [  5.5       ]]\n",
      "\n",
      " [[  3.66666667]\n",
      "  [  2.75      ]\n",
      "  [  2.75      ]]], shape=(2, 3, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "output = tf.matmul(attention_weights, V)  \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b951165-5137-4912-bd18-8f69a0522e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[277.75         3.66666667]\n",
      " [ 10.           2.75      ]\n",
      " [  5.5          2.75      ]]\n"
     ]
    }
   ],
   "source": [
    "output = np.transpose(output, (1, 0, 2))  # (seq_len_q, num_heads, v_dim)\n",
    "output = np.reshape(output, (-1, total_v_dim))  # (seq_len_q, total_v_dim)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8066d2-2153-4069-87c5-fc20127f3f31",
   "metadata": {},
   "source": [
    "## Multi-Head Self Attention\n",
    "\n",
    "Transformer의 Multi-Head Self Attention의 작동 방식은 다음과 같다.\n",
    "\n",
    "- 이전에 공부한 어텐션 메커니즘에서는 query가 디코더에서 제공되고, key와 value는 인코더에서 제공된다. 이는 디코더가 인코더의 출력과 상호작용하여 관련 정보를 추출하는 방식.\n",
    "\n",
    "- **셀프** 어텐션에서는 query, key, value를 모두 동일한 입력 시퀀스에서 생성한다. 즉, 인코더 입력 데이터 자체에서 query, key, value를 추출함.\n",
    "\n",
    "- 이 방식은 입력 시퀀스의 각 요소가 시퀀스 내의 다른 모든 요소와 상호작용하여, 자신의 문맥(context)을 이해하도록 도와준다.\n",
    "\n",
    "- **Multi-Head** attention은 이러한 셀프 어텐션을 여러 헤드로 병렬 처리하여, 서로 다른 서브스페이스의 정보를 학습할 수 있게 한다. 각 헤드는 입력 데이터의 다른 부분에 초점을 맞추어 보다 풍부한 표현을 학습하는 효과가 있다.\n",
    "\n",
    "- 이를 통해 모델은 시퀀스 내의 긴 의존성을 효과적으로 캡처할 수 있으며, 자연어 처리와 같은 분야에서 문맥을 이해하는 데 매우 유용하다.\n",
    "\n",
    "- Self-attentions에서는 자연스럽게 `X_q = X_k`, `Q = K`이 된다. 단, value는 key와 다르게 지정 가능\n",
    "\n",
    "케라스에서는 [`tensorflow.keras.MultiHeadAttention`](https://keras.io/api/layers/attention_layers/multi_head_attention/)를 이용하여 구현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599afdf5-1013-44df-ae0b-2ebde66448a1",
   "metadata": {},
   "source": [
    "### MultiHeadAttention 구조\n",
    "\n",
    "`MultiHeadAttention`에서의 작업 흐름은 다음과 같다.\n",
    "\n",
    "```\n",
    "Input Tensor → [Dense Layer for Query (W_q)] → Queries (Q)\n",
    "Input Tensor → [Dense Layer for Key (W_k)]   → Keys (K)\n",
    "Input Tensor → [Dense Layer for Value (W_v)] → Values (V)\n",
    "\n",
    "[Split into h heads]  (Q → Q₁...Q_h,  K → K₁...K_h,  V → V₁...V_h)\n",
    "\n",
    "[For each attention head]                                                                      \n",
    "     [Scaled Dot Product Attention] (Q_i · K_i^T / sqrt(depth)) → Attention Scores\n",
    "        → [Softmax] → Attention Weights\n",
    "        → [Dot Product] (Attention Weights, V_i)\n",
    "        → Attention Outputs (for each head)\n",
    "               ↓                       \n",
    "[Concatenate Heads] → Combined all Head Outputs → [Dense Layer] →  Output\n",
    "\n",
    "```\n",
    "\n",
    "Self multi head attention 결과는 입력 시퀀스의 복잡한 관계를 학습하여, 시퀀스 내의 문맥 정보를 풍부하게 표현한 결과라고 볼 수 있다.\n",
    "\n",
    "`MultiHeadAttention`의 주요 인자는 다음과 같다.\n",
    "\n",
    "- `num_heads` : 병렬 처리되는 head들의 수.  head는 input dimension을 나누어서 사용하기 때문에, 일반적으로 input dimension이 head의 수로 나누어떨어지도록 설정된다.\n",
    "  \n",
    "- `key_dim` : 각 head 별 query 및 key의 벡터 차원, 전체 query 및 key 벡터는 `num_heads * key_dim`의 차원을 가짐\n",
    "\n",
    "`MultiHeadAttention`의 call argument로서 `query`, `key`, `value`가 존재하는데, 이들은 각각 Q, K, V를 생성하는데 사용되는 입력 벡터 역할을 한다고 생각하면 된다.\n",
    "\n",
    "- 즉, $W$들이 곱해지기 전의 값.\n",
    "\n",
    "- 마지막 Dense layer는 Concatenate된 head의 결과를 모델의 기대하는 차원으로 투영하는 역할. Timedistrbuted로 처리 됨.\n",
    "\n",
    "  - `output_shape`을 따로 지정할 수도 있고, default로 `output_shape = None`일 경우 (쿼리) 입력 차원과 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114d9c11-6d06-40a5-8cb8-134364dae59d",
   "metadata": {},
   "source": [
    "### `MultiHeadAttention` 파라미터 분석\n",
    "\n",
    "간단하게 `key_dim = value_dim`이고, `output_shape`을 따로 설정하지 않은 상황, 즉, `output_dim = input_dim`에서 살펴보자.  \n",
    "\n",
    "- Q, K, V의 weight matrix들\n",
    "\n",
    "  - Q, K, V는 기본적으로 `input_dim` 차원의 벡터를 `num_heads * key_dim` 차원으로 보내는 역할\n",
    "  \n",
    "  - 각각 `(input_dim, num_heads * key_dim)` 크기의 행렬, 즉 각각 `input_dim * num_heads * key_dim`의 파라미터 수를 가짐.\n",
    "\n",
    "- Bias term for Q, K, V: 각각 `num_heads * key_dim` 개수의 bias term이 있음\n",
    "\n",
    "- Output projection dense layer 가중치 :\n",
    "\n",
    "  - Head output을 모두 concat한 후, 최종 출력 차원으로 다시 투영하는 dense layer : `(num_heads * value_dim) → output_dim`\n",
    "  \n",
    "    - 단, 아래 예제에서는 `key_dim = value_dim`을 가정 중  <br><br>\n",
    "  - W: shape = (`num_heads * key_dim`, `output_dim`), 보통 `output_dim = input_dim`\n",
    "\n",
    "  - bias : `output_dim`개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd22c2aa-7704-4aad-9e2d-707bebf2ba59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 6, 24)]      0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 6, 24)       2400        ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,400\n",
      "Trainable params: 2,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 이 값들을 변경해 가며 테스트해 보자.\n",
    "input_dim = 24\n",
    "key_dim = 3\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(timestep, input_dim))\n",
    "MHA_layer = layers.MultiHeadAttention(key_dim=key_dim, num_heads=num_heads)\n",
    "x = MHA_layer(encoder_inputs, encoder_inputs) # (query, value). key는 생략, 즉, key=query로 자동 설정\n",
    "model = keras.Model(encoder_inputs, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "751aa207-5355-4683-8b0a-cc2df9de3faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q, K, V 계산을 위한 W와 bias들 파라미터의 수\n",
    "3 * (input_dim * num_heads * key_dim + num_heads * key_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03b28572-3edd-402e-b5bb-53d49c6b3666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마지막 dense layer의 파라미터 수. output_dim = input_dim\n",
    "num_heads * key_dim * input_dim +  input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a50a2094-2097-4549-bfd6-f188e7760c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총합\n",
    "3 * (input_dim * num_heads * key_dim + num_heads * key_dim) + num_heads * key_dim * input_dim +  input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a5931-c43b-4dbd-81dc-bf3f9cc71d38",
   "metadata": {},
   "source": [
    "물론 `key_dim != value_dim`과 `input_dim != output_dim`인 경우도 파라미터 개수를 계산해 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "467256e4-8321-4031-8f17-3352177b700e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 6, 24)]      0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 6, 32)       3056        ['input_2[0][0]',                \n",
      " eadAttention)                                                    'input_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,056\n",
      "Trainable params: 3,056\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 이 값들을 변경해 가며 테스트해 보자.\n",
    "timestep = 6\n",
    "dim = 4\n",
    "\n",
    "input_dim = 24\n",
    "output_shape = 32  # output_dim\n",
    "key_dim = 3\n",
    "value_dim = 4\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(timestep, input_dim))\n",
    "MHA_layer = layers.MultiHeadAttention(key_dim=key_dim, value_dim=value_dim, num_heads=num_heads, output_shape=output_shape)\n",
    "x = MHA_layer(encoder_inputs, encoder_inputs)  # (query, value). key는 생략, 즉, key=query로 자동 설정\n",
    "model = keras.Model(encoder_inputs, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eda6b48b-9460-4123-ba01-104b13429666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3056"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파라미터 수 계산\n",
    "# W_q, W_k, W_v와 bias들\n",
    "pa = 2 * (input_dim * num_heads * key_dim + num_heads * key_dim) + (input_dim * num_heads * value_dim + num_heads * value_dim) \n",
    "# dense layer\n",
    "pd = num_heads * value_dim * output_shape +  output_shape\n",
    "pa + pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93989123-cbf8-4368-a18d-a5be283d1acf",
   "metadata": {},
   "source": [
    "### 간단 예제\n",
    "\n",
    "입력 시퀀스의 각 time step 벡터를 self-attention으로 처리하여, 그 결과를 그대로 예측하는 간단한 예제를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d967e421-b3f8-4973-a826-47b0e00e0cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 5, 12)]      0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 5, 12)       624         ['input_3[0][0]',                \n",
      " eadAttention)                                                    'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 5, 12)        156         ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 780\n",
      "Trainable params: 780\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터\n",
    "seq_len = 5\n",
    "d_model = 12\n",
    "num_heads = 4\n",
    "key_dim = d_model // num_heads\n",
    "batch_size = 32\n",
    "\n",
    "# 입력 데이터: 랜덤 시퀀스\n",
    "X = np.random.randn(1000, seq_len, d_model).astype(np.float32)\n",
    "Y = X.copy()  # 목표는 그대로 복사\n",
    "\n",
    "# 모델 구성\n",
    "inputs = tf.keras.Input(shape=(seq_len, d_model))\n",
    "\n",
    "# MHA Layer\n",
    "attention_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(inputs, inputs)\n",
    "\n",
    "# Optional: Dense layer (identity map)\n",
    "outputs = layers.Dense(d_model)(attention_out)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "285e5736-a7cb-42e1-b3f8-f30489ef96af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.9979 - val_loss: 0.9633\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.9468 - val_loss: 0.9163\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.8920 - val_loss: 0.8597\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.8263 - val_loss: 0.7928\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.7511 - val_loss: 0.7140\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.6683 - val_loss: 0.6266\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5835 - val_loss: 0.5429\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5087 - val_loss: 0.4785\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4524 - val_loss: 0.4305\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4042 - val_loss: 0.3859\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.3593 - val_loss: 0.3489\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.3261 - val_loss: 0.3225\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3053 - val_loss: 0.3047\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2908 - val_loss: 0.2906\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2778 - val_loss: 0.2767\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2632 - val_loss: 0.2599\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2452 - val_loss: 0.2388\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2249 - val_loss: 0.2205\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2104 - val_loss: 0.2109\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2037 - val_loss: 0.2058\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1994 - val_loss: 0.2019\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1961 - val_loss: 0.1990\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1934 - val_loss: 0.1963\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1910 - val_loss: 0.1942\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1892 - val_loss: 0.1927\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1873 - val_loss: 0.1908\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1854 - val_loss: 0.1890\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 0.1869\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1814 - val_loss: 0.1845\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1782 - val_loss: 0.1803\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1732 - val_loss: 0.1738\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1642 - val_loss: 0.1613\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1493 - val_loss: 0.1432\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1310 - val_loss: 0.1273\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1202 - val_loss: 0.1215\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1170 - val_loss: 0.1200\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1155 - val_loss: 0.1189\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1142 - val_loss: 0.1179\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1134 - val_loss: 0.1174\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1126 - val_loss: 0.1168\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1119 - val_loss: 0.1165\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1113 - val_loss: 0.1160\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1108 - val_loss: 0.1157\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1103 - val_loss: 0.1154\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1099 - val_loss: 0.1151\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1095 - val_loss: 0.1147\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1091 - val_loss: 0.1145\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1088 - val_loss: 0.1143\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1086 - val_loss: 0.1141\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 0.1140\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1081 - val_loss: 0.1142\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1078 - val_loss: 0.1139\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1076 - val_loss: 0.1136\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1073 - val_loss: 0.1137\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1073 - val_loss: 0.1134\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1070 - val_loss: 0.1135\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1068 - val_loss: 0.1132\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1067 - val_loss: 0.1134\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1066 - val_loss: 0.1137\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1064 - val_loss: 0.1132\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1062 - val_loss: 0.1133\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1061 - val_loss: 0.1133\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1060 - val_loss: 0.1132\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1059 - val_loss: 0.1132\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1057 - val_loss: 0.1132\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1056 - val_loss: 0.1130\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1055 - val_loss: 0.1131\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1054 - val_loss: 0.1129\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1053 - val_loss: 0.1130\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1053 - val_loss: 0.1128\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1052 - val_loss: 0.1127\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1050 - val_loss: 0.1128\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1049 - val_loss: 0.1126\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1048 - val_loss: 0.1128\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1047 - val_loss: 0.1128\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1046 - val_loss: 0.1127\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1046 - val_loss: 0.1128\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1045 - val_loss: 0.1128\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 0.1129\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 0.1128\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1042 - val_loss: 0.1127\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1042 - val_loss: 0.1128\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1041 - val_loss: 0.1130\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1040 - val_loss: 0.1129\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1039 - val_loss: 0.1129\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1039 - val_loss: 0.1132\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1038 - val_loss: 0.1129\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1038 - val_loss: 0.1130\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1037 - val_loss: 0.1131\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1035 - val_loss: 0.1130\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1034 - val_loss: 0.1131\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1034 - val_loss: 0.1129\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 0.1132\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1032 - val_loss: 0.1138\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1035 - val_loss: 0.1133\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1031 - val_loss: 0.1136\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1030 - val_loss: 0.1137\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1029 - val_loss: 0.1139\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 0.1138\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 0.1136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cb984cc520>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "model.fit(X, Y, epochs=100, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdc8476c-5336-4b37-bdd7-b249f822525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "Input:\n",
      "[[ 0.83  1.31 -1.45 -0.65 -0.67 -0.12  0.92 -1.6  -1.03  2.05  0.31 -0.36]\n",
      " [-1.69  0.99  0.39  1.42  0.05  1.58 -0.74 -0.28 -2.58  0.98  0.3  -1.03]\n",
      " [ 0.54 -0.36  1.66  0.4  -0.24  0.41  0.2  -0.66  1.31  0.81 -0.56  0.74]\n",
      " [-1.07 -0.43 -1.47 -0.05 -0.11 -1.06 -1.71 -0.65  0.8   0.61 -0.11 -1.04]\n",
      " [-0.54 -0.95 -0.66  1.58 -0.17 -0.7  -0.07 -0.09 -1.74  0.38 -0.07 -1.26]]\n",
      "Prediction:\n",
      "[[ 0.25  1.6  -1.82  0.05 -0.73  0.35  0.05 -1.66 -1.11  2.46  0.65 -0.93]\n",
      " [-1.74  0.37  0.51  1.12 -0.44  1.17 -0.42 -0.53 -2.52  1.02  0.16 -1.63]\n",
      " [ 0.05 -0.71  1.24  0.46  0.14  0.71  0.96 -0.54  1.05  1.3  -0.15  0.8 ]\n",
      " [-1.27 -0.09 -1.09 -0.05 -0.46 -0.88 -2.1  -0.51 -0.13  1.11  0.96 -1.26]\n",
      " [-0.83 -0.05 -0.5   1.1  -0.5  -0.23 -0.27 -0.43 -2.26  0.67  0.36 -1.65]]\n"
     ]
    }
   ],
   "source": [
    "test_input = np.random.randn(1, seq_len, d_model).astype(np.float32)\n",
    "pred = model.predict(test_input)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(np.round(test_input[0], 2))\n",
    "\n",
    "print(\"Prediction:\")\n",
    "print(np.round(pred[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb9615-9cb2-4101-a34a-a2e1f581b7ec",
   "metadata": {},
   "source": [
    "## 인코더\n",
    "\n",
    "인코더 모델을 구성하는 함수를 정의하자. \n",
    "\n",
    "인코더 모델은 self-attetion 부분과 feed-forward 부분으로 구성된다.\n",
    "\n",
    "- 앞에서 공부한 인코더-디코더 모델에서 순환 신경망 대신 self-attention이 있는 형태\n",
    "\n",
    "Feed-forward 부분은 self-attention 메커니즘 이후에 입력 데이터에 대한 추가적인 비선형 변환을 수행하는 단계이다.\n",
    "\n",
    "이는 모델이 더 복잡하고 다양한 패턴을 학습할 수 있도록 도와준다. \n",
    "\n",
    "Transformer encoder의 feed-forward 부분은 일반적으로 두 개의 Dense 레이어와 활성화 함수로 구성된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4038d99-8b12-4b6e-80c4-8adba03d1273",
   "metadata": {},
   "source": [
    "\n",
    "### 인코더의 주요 구성\n",
    "\n",
    "Transformer의 인코더는 하나의 블록 안에 다음과 같은 주요 구성요소를 갖는다.\n",
    "\n",
    "1. Multi-Head Self-Attention Layer\n",
    "\n",
    "   * Transformer는 문장을 처리하기 위해 개발된 것으로, self-attention은 입력 시퀀스 내에서 단어들 간의 상호관계를 학습\n",
    "   * 같은 시퀀스 안에서 query, key, value가 동일하기 때문에 self-attention이라고 부름 <br><br>\n",
    "\n",
    "2. Layer Normalization\n",
    "   * 각 샘플의 각 time step 별로 해당 feature 차원(`axis=-1`)에 대해 평균과 분산을 계산하여 정규화하는 과정 → 학습 안정화, 수렴 향상\n",
    "   * Transformer에서는 Residual 연결 이후 또는 이전에 항상 LayerNormalization을 적용 <br><br>\n",
    "\n",
    "3. Residual connection : attention ouput + input\n",
    "   * Attention의 결과에 original 입력을 더해주는 과정을 residual connection이라고 부른다.\n",
    "   * `inputs`까지 직접적인 경로(identity path)를 만들어서, 역전파 시 gradient가 입력까지 잘 전달되게 함\n",
    "   * 완전히 새로 생성된 출력을 쓰기보단 기존 정보 + 조정된 정보로 자연스럽게 업데이트 <br><br>\n",
    "\n",
    "4. Position-wise Feed-Forward Network (FFN)\n",
    "\n",
    "   * 각 위치의 출력 벡터를 독립적으로 변환\n",
    "   * 일반적으로 `Dense → ReLU → Dropout → Dense` \n",
    "   * 보통 첫 번째 dense layer에서 차원이 확장되고, 두 번째 dense layer에서 원래 차원으로 복원 <br><br>\n",
    "\n",
    "5. 마지막으로 Residual connection 한 번 더 수행\n",
    "   * 역전파 시 Gradient 흐름을 원활하게 함\n",
    "   * 초기엔 FFN이 거의 학습되지 않아도 residual은 그대로 전달하여 정보 손실 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8638fe-dac6-46a1-acc2-fd5e5b831687",
   "metadata": {},
   "source": [
    "아래 인코더 모델의 텐서 흐름은 다음과 같다.\n",
    "\n",
    "```\n",
    "Input → [Layer Normalization] → [Multi-Head Attention] → [Dropout] → self-attention result\n",
    "    ↓      \n",
    "→ [Sum](Input + self-attention result) → <Residual>\n",
    "\n",
    "→ [Layer Normalization] → [Dense (ReLU)] → [Dropout] → [Dense] → Feedforward result  \n",
    "                                          \n",
    "→ [Sum](<Residual> + Feedforward result) → Encoder output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "096d1068-f6e7-48d9-a977-f559cba3f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 모델 구성\n",
    "def transformer_encoder(inputs, num_heads, key_dim, ff_dim, dropout=0.1):\n",
    "    # === Self-Attention Block ===\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    MHA_layer = layers.MultiHeadAttention(key_dim=key_dim, num_heads=num_heads, dropout=dropout)\n",
    "    \n",
    "    x = MHA_layer(query=x, value=x, key=x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "     # === Feed-Forward Block ===\n",
    "    ff = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    ff = layers.Dense(units=ff_dim, activation=\"relu\")(ff)  # ff_dim으로 확장\n",
    "    ff = layers.Dropout(dropout)(ff)\n",
    "    ff = layers.Dense(units=inputs.shape[-1])(ff) # 원래 차원으로 복원\n",
    "    return ff + res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff0e10-8b8b-4308-8bfc-079bd89f7ba7",
   "metadata": {},
   "source": [
    "적절한 input tensor를 생성하여 `transformer_encoder`에 입력하면 output tensor를 얻게 되어 추후에 모델을 생성할 때 이용할 수 있다.\n",
    "\n",
    "예를 들어, 다음과 같은 코드를 보라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c9a49dd-1010-4218-95ef-c3edcc7bacf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 6, 4) dtype=float32 (created by layer 'tf.__operators__.add_1')>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs = keras.Input(shape=(timestep, dim))\n",
    "\n",
    "encoder_ouputs = transformer_encoder(encoder_inputs, num_heads = 2, key_dim = 8, ff_dim = 10, dropout = 0.1)\n",
    "encoder_ouputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93ed119-845d-4c7a-b072-ca3d60bde68c",
   "metadata": {},
   "source": [
    "## 디코더\n",
    "\n",
    "디코더는 인코더와 비슷하나, 어텐션이 다음 세 단계에 걸쳐 이루어 진다.\n",
    "\n",
    "- **Masked self attention**\n",
    "  - 디코더가 순차적으로 값을 생성할 때 현재까지 생성된 값들만을 이용하여 attention을 수행한다.\n",
    "  - 즉, 미래에 디코더가 생성할 값은 attention 계산에 사용하지 않겠다는 의미이다.<br><br>\n",
    "    \n",
    "- Encoder-Decoder attention\n",
    "  - 디코더 출력과 인코더 출력 간의 attention   \n",
    "  - 디코더가 특정 시점 값을 생성할 때 인코더 출력 전체를 살펴보므로 masked attention은 아니다. <br><br>\n",
    " \n",
    "- Position-wise Feedforward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71880d0-63f7-4710-a72f-16d808a75450",
   "metadata": {},
   "source": [
    "### Mask 행렬\n",
    "\n",
    "마스크 행렬은 어디를 가릴지 위치를 나태내 주는 행렬.\n",
    "\n",
    "- 가려야 할 위치가 1이고, 나머지는 0.\n",
    "- 가장 기본적으로 upper triangular part가 1이고 나머지는 0인 형태가 새용됨\n",
    "\n",
    "Transformer에서 attention mask는 `softmax`에 들어가기 전 score 행렬 `QKᵀ`에 다음과 같이 적용된다.\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{Q}, \\mathbf K, \\mathbf V) = \\text{softmax} \\left( \\frac{\\mathbf Q \\mathbf{K}^\\top}{\\sqrt{d_k}} +  \\text{mask} \\cdot(-\\infty )\\right) \\mathbf{V}\n",
    "$$\n",
    "\n",
    "- 즉, softmax를 적용했을 때, 0이 되게 하기 위함\n",
    "  \n",
    "- 실제로는 $-\\infty$를 곱할 수 없기 때문에, `-1e9`와 같은 매우 작은 수를 곱함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1030d8c-c471-4c6c-8560-4036d962dcaa",
   "metadata": {},
   "source": [
    "Mask 행렬을 위해 다음 함수를 정의하자.\n",
    "\n",
    "여기서 `size`는 `timestep`에 해당 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bee48567-8b85-43e7-9f75-3de7c2e68086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 6), dtype=float32, numpy=\n",
       "array([[0., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    # band_part(A, -1, 0)은 lower triangular 행렬을 추출\n",
    "    return 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "create_look_ahead_mask(timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97750681-7476-4f25-8209-23122104f5f6",
   "metadata": {},
   "source": [
    "활용 시에는 아래와 같이 인자로 전달한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "907cf60f-f466-49fc-9c37-6bebb1206207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6, 4), dtype=float32, numpy=\n",
       "array([[[-0.10585935, -0.28205392,  0.07306184,  0.33768246],\n",
       "        [-0.00079814, -0.00212657,  0.00055086,  0.00254598],\n",
       "        [-0.07221033, -0.19239873,  0.04983801,  0.23034488],\n",
       "        [ 0.02255827,  0.06010474, -0.01556923, -0.07195899],\n",
       "        [ 0.06998947,  0.18648142, -0.04830521, -0.2232605 ],\n",
       "        [-0.07444675, -0.19835751,  0.05138154,  0.23747888]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.random.randn(1, 3, 4)\n",
    "x = np.concatenate([temp, temp], axis = 1)\n",
    "\n",
    "layers.MultiHeadAttention(num_heads=1, key_dim=1)(query=x, value=x, attention_mask=create_look_ahead_mask(timestep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221548d-8e10-40fc-9479-bbaef24c3a66",
   "metadata": {},
   "source": [
    "아래 디코더 모델의 텐서 흐름은 다음과 같다.\n",
    "\n",
    "```\n",
    "Decoder Input  → [Layer Normalization] → [Masked Self Multi-Head Attention] → [Dropout] → self-attention result\n",
    "    ↓\n",
    "→ [Sum](Decoder Input + self-attention result) → <Residual1>\n",
    "                                                                 \n",
    "Encoder Output → [Encoder-Decoder Multi-Head Attention with Encoder Output] → [Dropout] → Encoder-Decoder attention result\n",
    "\n",
    "→ [Sum](<Residual1> + Encoder-Decoder attention result) → <Residual2>\n",
    "        \n",
    "→ [Layer Normalization] → [Dense (ReLU)] → [Dropout] → [Dense] → Feedforward result  \n",
    "                                          \n",
    "→ [Sum](<Residual2> + Feedforward result) → Decoder output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79098ae4-7f9d-4a98-a059-73d395f90247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 디코더 블록\n",
    "def transformer_decoder(inputs, encoder_output, num_heads, key_dim, ff_dim, dropout=0.1):\n",
    "    seq_len = tf.shape(inputs)[1]\n",
    "    look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "    # Masked Multi-Head Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    MHA_layer1 = layers.MultiHeadAttention(key_dim=key_dim, num_heads=num_heads, dropout=dropout)\n",
    "    x = MHA_layer1(query=x, value=x, key=x, attention_mask=look_ahead_mask)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Encoder-Decoder Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    MHA_layer2 = layers.MultiHeadAttention(key_dim=key_dim, num_heads=num_heads, dropout=dropout)\n",
    "    x = MHA_layer2(query=x, value=encoder_output, key=encoder_output)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + res\n",
    "\n",
    "    # Feed Forward Part\n",
    "    ff = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    ff = layers.Dense(units=ff_dim, activation=\"relu\")(ff)\n",
    "    ff = layers.Dropout(dropout)(ff)\n",
    "    ff = layers.Dense(units=inputs.shape[-1])(ff)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b05eeb-0882-4b76-ae06-10fd6940eb9c",
   "metadata": {},
   "source": [
    "## Transformer 모델 구성\n",
    "\n",
    "위에서 정의된 인코더 디코더를 이용하여 transformer 모형을 구성해 보겠다.\n",
    "\n",
    "`build_transformer_model` 함수 내에서 반복문을 이용하여 encoder와 decoder를 여러 블록으로 설정할 수 있다.\n",
    "\n",
    "- 각 블록은 입력 데이터에 대한 더 복잡한 패턴과 관계를 학습할 수 있도록 도와주고,\n",
    "- 깊은 구조는 멀리 떨어진 토큰 간의 복잡한 관계도 포착한다고 알려져 있다.\n",
    "\n",
    "전체 모델 구조는 다음과 같이 간단히 나타낼 수 있다. \n",
    "    \n",
    "```\n",
    "[Encoder Input] ─► PositionalEncoding ─► [Encoder Blocks] ─► encoder_outputs\n",
    "                                                                  ↓\n",
    "[Decoder Input] ─► PositionalEncoding ─► [Decoder Blocks using encoder_outputs] ─► Final Dense Layer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6df34e47-1e62-46f1-b12a-ed7c69ea54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_model(input_shape, key_dim, num_heads, ff_dim, num_encoder_blocks, num_decoder_blocks, dropout=0.1):\n",
    "    encoder_inputs = keras.Input(shape=input_shape)\n",
    "    x = PositionalEncoding(input_shape[0], input_shape[1])(encoder_inputs)\n",
    "    for _ in range(num_encoder_blocks):\n",
    "        x = transformer_encoder(x, key_dim, num_heads, ff_dim, dropout)\n",
    "    encoder_outputs = x\n",
    "\n",
    "    #decoder_inputs = keras.Input(shape=(input_shape[0] + 1, input_shape[1]))\n",
    "    decoder_inputs = keras.Input(shape=(None, input_shape[1]))\n",
    "    x = PositionalEncoding(input_shape[0] + 1, input_shape[1])(decoder_inputs)\n",
    "    for _ in range(num_decoder_blocks):\n",
    "        x = transformer_decoder(x, encoder_outputs, key_dim, num_heads, ff_dim, dropout)\n",
    "\n",
    "    outputs = layers.Dense(input_shape[1], activation=\"linear\")(x)\n",
    "    return keras.Model([encoder_inputs, decoder_inputs], outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11976e30-1124-4587-a58e-2c0e3c9649e3",
   "metadata": {},
   "source": [
    "간단한 트랜스포머 모델을 구현하여 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9297069b-a42f-4930-b5d5-d72d89b69ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 6, 12)]      0           []                               \n",
      "                                                                                                  \n",
      " positional_encoding_1 (Positio  (None, 6, 12)       0           ['input_5[0][0]']                \n",
      " nalEncoding)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 6, 12)       24          ['positional_encoding_1[0][0]']  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 6, 12)       624         ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]',  \n",
      "                                                                  'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 6, 12)        0           ['multi_head_attention_5[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 6, 12)       0           ['dropout_3[0][0]',              \n",
      " mbda)                                                            'positional_encoding_1[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 6, 12)       24          ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 6, 64)        832         ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 6, 64)        0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, None, 12)]   0           []                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 6, 12)        780         ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " positional_encoding_2 (Positio  (None, None, 12)    0           ['input_6[0][0]']                \n",
      " nalEncoding)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 6, 12)       0           ['dense_4[0][0]',                \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLambda  (3,)                0           ['positional_encoding_2[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 6, 12)       24          ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  ()                  0           ['tf.compat.v1.shape[0][0]']     \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 6, 12)       624         ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]',  \n",
      "                                                                  'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " tf.ones (TFOpLambda)           (None, None)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.__operators__.getitem[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 6, 12)        0           ['multi_head_attention_6[0][0]'] \n",
      "                                                                                                  \n",
      " tf.linalg.band_part (TFOpLambd  (None, None)        0           ['tf.ones[0][0]']                \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 6, 12)       0           ['dropout_5[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, None, 12)    24          ['positional_encoding_2[0][0]']  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLambda)  (None, None)         0           ['tf.linalg.band_part[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 6, 12)       24          ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, None, 12)    624         ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'tf.math.subtract[0][0]',       \n",
      "                                                                  'layer_normalization_6[0][0]',  \n",
      "                                                                  'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 6, 64)        832         ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, None, 12)     0           ['multi_head_attention_7[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 6, 64)        0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, None, 12)    0           ['dropout_7[0][0]',              \n",
      " mbda)                                                            'positional_encoding_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 6, 12)        780         ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, None, 12)    24          ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 6, 12)       0           ['dense_6[0][0]',                \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, None, 12)    624         ['layer_normalization_7[0][0]',  \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]', \n",
      "                                                                  'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, None, 12)     0           ['multi_head_attention_8[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, None, 12)    0           ['dropout_8[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, None, 12)    0           ['dropout_8[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_7[0][0]'] \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_1 (TFOpLamb  (3,)                0           ['tf.__operators__.add_8[0][0]'] \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  ()                  0           ['tf.compat.v1.shape_1[0][0]']   \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.ones_1 (TFOpLambda)         (None, None)         0           ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.linalg.band_part_1 (TFOpLam  (None, None)        0           ['tf.ones_1[0][0]']              \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, None, 12)    24          ['tf.__operators__.add_8[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.math.subtract_1 (TFOpLambda  (None, None)        0           ['tf.linalg.band_part_1[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, None, 12)    624         ['layer_normalization_9[0][0]',  \n",
      " eadAttention)                                                    'tf.math.subtract_1[0][0]',     \n",
      "                                                                  'layer_normalization_9[0][0]',  \n",
      "                                                                  'layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, None, 12)     0           ['multi_head_attention_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, None, 12)    0           ['dropout_10[0][0]',             \n",
      " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, None, 12)    24          ['tf.__operators__.add_9[0][0]'] \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, None, 12)    624         ['layer_normalization_10[0][0]', \n",
      " HeadAttention)                                                   'tf.__operators__.add_5[0][0]', \n",
      "                                                                  'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, None, 12)     0           ['multi_head_attention_10[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, None, 12)    0           ['dropout_11[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, None, 12)    0           ['dropout_11[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, None, 12)     156         ['tf.__operators__.add_11[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,316\n",
      "Trainable params: 7,316\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 하이퍼파라미터\n",
    "timestep = 6\n",
    "dim = 12\n",
    "num_heads = 4\n",
    "key_dim = dim // num_heads\n",
    "\n",
    "ff_dim = 64\n",
    "num_encoder_blocks = 2\n",
    "num_decoder_blocks = 2\n",
    "dropout = 0.1\n",
    "\n",
    "input_shape = (timestep, dim)\n",
    "model = build_transformer_model(input_shape, key_dim, num_heads, ff_dim, num_encoder_blocks, num_decoder_blocks, dropout)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6592dccf-d8b0-47da-8b35-76d2ac943f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bca0ca-c3fc-4b75-a6e3-ca545fb3fdc1",
   "metadata": {},
   "source": [
    "### 훈련 데이터\n",
    "\n",
    "`input_sequences`: `(5000, timestep, dim)` 크기의 랜덤 시퀀스 데이터를 생성하여 학습용 인코더 입력으로 사용해 보자.\n",
    "\n",
    "또한 디코더 시작 토큰(`start token`) 역할을 하는 0 벡터를 이용하여 `decoder_input_sequences`를 생성한다.\n",
    "\n",
    "* 디코더 입력 = 시작 토큰 + 인코더 입력 시퀀스  \n",
    "* 타깃 시퀀스 = 인코더 입력 시퀀스 + 끝 토큰  \r\n",
    "* 즉, 입력을 한 칸 오른쪽으로 shift한 형태\n",
    "  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d267137b-de92-47ee-be01-7c0c756c467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터: 랜덤 시퀀스\n",
    "num_samples = 5000\n",
    "input_sequences = np.random.randn(num_samples, timestep, dim).astype(np.float32)\n",
    "token = np.zeros((num_samples, 1, dim), dtype=np.float32)\n",
    "decoder_input_sequences = np.concatenate([token, input_sequences], axis=1)\n",
    "target_sequences = np.concatenate([input_sequences, token], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf061d6-3def-491e-9277-593db4f32bad",
   "metadata": {},
   "source": [
    "### 훈련 과정\n",
    "\n",
    "* 인코더 입력 = `input_sequences`\n",
    "* 디코더 입력 = `decoder_input_sequences`\n",
    "* 출력(타깃): `target_sequences`\n",
    "  \n",
    "모델이 다음 시점 토큰을 예측하도록 학습한다.\n",
    "\n",
    "Transformer seq2seq 모델을 자기회귀(auto-regressive) 학습 형태로 훈련하는 예제이며, 코더는 이전까지의 토큰(start token 포함)을 입력받아 다음 토큰을 맞추도록 학습한다.\n",
    "\n",
    "이 Transformer 모형은 시계열 예측 문제를 잘 학습하는 편은 아니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f464d52c-56f2-4a36-be85-5edfccfad9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "157/157 [==============================] - 3s 12ms/step - loss: 1.2731\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.8722\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.7736\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.7150\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.6711\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.6409\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.6218\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.6064\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.5965\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.5880\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.5797\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5753\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5709\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5643\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5610\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5574\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5563\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5562\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.5557\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5556\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.5484\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5431\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5399\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5356\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.5332\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5309\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5212\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5185\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5228\n",
      "Epoch 30/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5205\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.5165\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.5085\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5071\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5014\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4987\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4977\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4907\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4927\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4970\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4974\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5007\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4941\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4917\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4954\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4973\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4946\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4971\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4949\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4947\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.4958\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.4947\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.4922\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4928\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4905\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4917\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.4948\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.4918\n",
      "Epoch 58/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4930\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4924\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4920\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4903\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4852\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4887\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4847\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.4859\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4841\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4828\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4816\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4808\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4789\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4767\n",
      "Epoch 72/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4803\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4753\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4771\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.4732\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.4723\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.4721\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.4713\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4728\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4720\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4729\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4667\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4653\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4678\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4678\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4684\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4750\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.4661\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4699\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4714\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4677\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.4653\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4652\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4697\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.4638\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.4644\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4627\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4620\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.4646\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cbc3498e20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 학습\n",
    "model.fit(\n",
    "    [input_sequences, decoder_input_sequences],\n",
    "    target_sequences,\n",
    "    epochs=100,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db84d91-55bd-42e7-94ec-a3694ec6d3e1",
   "metadata": {},
   "source": [
    "### 예측 과정\r\n",
    "\r\n",
    "Autoregressive하게 예측하는 과정을 살펴보자.\r\n",
    "\r\n",
    "* 인코더 입력(`test_input`)을 모델에 고정시켜 둠.\r\n",
    "* 디코더 입력(`decoder_input`)은 처음에는 `[0]` 토큰 하나로 시작.\r\n",
    "* 한 스텝씩 모델의 출력을 얻고, 해당 시점의 토큰을 디코더 입력에 추가.\r\n",
    "* 이렇게 autoregressive(자기회귀) 방식으로 전체 시퀀스를 생성.\r\n",
    "\r\n",
    "훈련이 잘 된 것이 아니라 시계열 예측이 잘 되고 있지는 않지만 일반적인 예측 과정을 공부하는 차원에서 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97cc49af-32a0-4e6a-a00a-5f21378ce30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "test_input = np.random.uniform(0, 100, size=(1, timestep, dim)).astype(np.float32)\n",
    "decoder_input = np.zeros((1, 1, dim), dtype=np.float32)\n",
    "\n",
    "for t in range(timestep):\n",
    "    out = model.predict([test_input, decoder_input], verbose=0)\n",
    "    next_token = out[:, t:t+1, :]  # 현재 t 위치의 출력\n",
    "    predicted.append(next_token)\n",
    "    decoder_input = np.concatenate([decoder_input, next_token], axis=1)\n",
    "\n",
    "predicted_seq = np.concatenate(predicted, axis=1)  # [1, T, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3acd8ef6-71d4-4cd9-adbe-30598718e098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Input Sequence ===\n",
      "[[ 5.9  84.48 55.46 60.91 39.75 25.66 16.88 49.7  13.09 82.83 67.29 23.87]\n",
      " [86.38 84.48 41.02 54.33 39.05  2.61  2.98 88.1  59.53 55.66 72.29 91.86]\n",
      " [59.89 24.55 53.84 14.91 79.9  28.69 64.5  68.98 22.69 35.47 49.43  2.2 ]\n",
      " [82.73  7.92 74.   25.94 80.67 77.67 52.23 57.15 92.46 86.11 11.56 82.25]\n",
      " [95.52 74.13 31.54 23.44 96.42 39.99 29.29 37.63 14.87 42.31 96.79 14.39]\n",
      " [83.46 78.55 85.41 36.36 99.    4.62 73.88 71.37 34.87 70.11 34.02 65.76]]\n",
      "\n",
      "=== Predicted Sequence ===\n",
      "[[51.62 48.87 56.84 26.29 50.04 37.92 33.41 48.82 41.83 41.78 43.68 34.21]\n",
      " [48.66 50.31 48.82 41.38 34.74 36.19 35.28 42.52 48.47 42.86 47.28 41.06]\n",
      " [45.79 49.87 58.9  27.82 43.88 33.59 35.59 42.36 46.26 35.82 42.53 28.67]\n",
      " [42.78 48.1  48.96 38.   39.62 38.08 35.93 46.78 46.47 41.95 47.12 38.49]\n",
      " [52.76 52.65 55.94 32.61 49.91 43.06 47.63 34.98 44.82 46.53 42.54 46.07]\n",
      " [46.69 50.85 56.6  31.84 40.4  33.5  35.04 43.13 46.62 36.85 44.41 31.41]]\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "print(\"=== Input Sequence ===\")\n",
    "print(np.round(test_input[0], 2))   # 원래 입력 시퀀스\n",
    "\n",
    "print(\"\\n=== Predicted Sequence ===\")\n",
    "print(np.round(predicted_seq[0], 2))  # 모델이 step-by-step으로 생성한 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
